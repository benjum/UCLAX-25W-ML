{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4edc1037-9b05-44c6-8c07-219f69328b77",
   "metadata": {},
   "source": [
    "# Topic modeling\n",
    "\n",
    "We are going to look at data from the [20 Newsgroups](http://qwone.com/~jason/20Newsgroups/) dataset.  These are postings to newsgroups in 20 different categories.\n",
    "\n",
    "Scikit-learn has a function for downloading the data.  See: https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_20newsgroups.html\n",
    "\n",
    "## Supervised Learning with Naive Bayes\n",
    "\n",
    "We are going to start out using Naive Bayes for supervised ML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011b5d42-d37e-4e71-ab13-d99e91048d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fe4dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ntrain = fetch_20newsgroups(subset='train')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b5656c",
   "metadata": {},
   "source": [
    "Here is the description of the dataset, along with an interesting example and tidbits of advice: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59427235",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ntrain.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b56a644",
   "metadata": {},
   "source": [
    "An example of the first five newsgroup posts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0411f672",
   "metadata": {},
   "outputs": [],
   "source": [
    "ntrain.data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e8eb40",
   "metadata": {},
   "source": [
    "We have 11314 newsgroup postings, with 20 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e24e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "ntrain.target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d327bd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "ntrain.target[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f153a9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "[ntrain.target_names[i] for i in ntrain.target[:10]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012bc791",
   "metadata": {},
   "source": [
    "We'll keep it a bit simpler for the moment and only consider two of the categories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b3c1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "cats = ['sci.space', 'comp.graphics']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a70e617",
   "metadata": {},
   "source": [
    "Re-import the training data for just these two categories, along with test data too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca5fb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "ntrain = fetch_20newsgroups(subset='train', categories=cats)\n",
    "ntest = fetch_20newsgroups(subset='test',categories=cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d69617",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ntrain.data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffbb210",
   "metadata": {},
   "source": [
    "How does this compare with what I showed on the slide?  And does it matter that it's different?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdde4ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ntrain.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f293027",
   "metadata": {},
   "outputs": [],
   "source": [
    "ntrain.target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32c21a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ntrain.target[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713d7020",
   "metadata": {},
   "outputs": [],
   "source": [
    "ntest.target.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d30db1b-c6bf-4496-8fa9-5e518457b459",
   "metadata": {},
   "source": [
    "## How can we think of text as numbers for quantitative analysis?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d76b054-c5fe-4878-b588-ecbad9afeae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fddd4b7-7e04-4a50-9a53-b77eddd3c4aa",
   "metadata": {},
   "source": [
    "## Bag-of-Words (BoW)\n",
    "\n",
    "BoW represents a document as a set of words without regard for word order.  Each word is assigned a unique index, and a document is represented as a vector whose values at the index for each word are the word counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5a52ef-02c3-496c-9d84-0c32b1f3a69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\"The cat slept and then meowed.\", \n",
    "          \"The tiger slept and then roared.\", \n",
    "          \"The boy ran home and then the boy laughed.\"]\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "X = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2209712-e12e-463e-a19f-693a767d2cae",
   "metadata": {},
   "source": [
    "Even though we are using Scikit-Learn to do the CountVectoriz-ing, there is no reason that we couldn't manually do it ourselves too with a bit of Python.  It's just convenient to do it the Scikit-Learn way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ae02a6-aaef-4224-85ba-822a61dc3f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba16538-7aef-4642-b214-b01300d2a4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(X.toarray(), \n",
    "             columns=vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c580f142-4666-4893-afa6-d2f2a9f03db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# as to compare against our corpus:\n",
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551dcf74-cdba-438b-a1f2-8334f584f681",
   "metadata": {},
   "source": [
    "## Term Frequency-Inverse Document Frequency (TF-IDF)\n",
    "\n",
    "TF-IDF extends BoW by accounting for the uniqueness of words in distinguishing between documents.  The word counts of BoW are weighted by words' relative rarity across the entire corpus.\n",
    "\n",
    "* Scikit-Learn's TF-IDF calculation is [described here](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html#sklearn.feature_extraction.text.TfidfTransformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62511aa-8ebc-40bd-9793-734f54426dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "X_tfidf = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4295bf5e-2007-419a-aeef-cc42d7aee34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(X_tfidf.toarray(), \n",
    "             columns=vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83203d5-092a-4a79-8f22-a97e85d99f00",
   "metadata": {},
   "source": [
    "There are a lot of mathematical details that come in here for trying to get well behaved forms of TF-IDF, and it's actually a messy business trying to back this out from the word counts and frequencies.\n",
    "\n",
    "You can ignore the following if you want to, but here is how one would go directly from the matrix of counts to scikit-learn's version of the TFIDF measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b2b8c7-d25d-4d0e-84c8-fbc50c0fcdfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_bow = pd.DataFrame(X.toarray(), \n",
    "             columns=vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12cbcc9-035c-4f07-85e2-cec9d2e6a01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abe87e0-bc71-43b6-ac26-a6c9372fdd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the term frequencies in each of the three documents\n",
    "(x_bow.T / x_bow.T.sum(axis=0)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f36502b-e55c-49fb-b90c-19f4040480f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the number of documents in which each word occurs\n",
    "(x_bow > 0).sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083cd42d-7395-4280-8248-218bd014d99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = (x_bow.T / x_bow.T.sum(axis=0)).T\n",
    "\n",
    "# the +1 at the end is so that even words that occur across all docs\n",
    "# still have a non-zero TFIDF\n",
    "# the +1 in numerator and +1 in denominator are conveniences to\n",
    "# handle the otherwise division by 0 for words that have 0 counts\n",
    "idf = np.log((1+3) / (1+(x_bow > 0).sum(axis=0))) + 1\n",
    "\n",
    "tf * idf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632ddf6c-9838-421c-bf55-48646d50a3ff",
   "metadata": {},
   "source": [
    "... and then one has to do a cosine normalization (the squares of elements in the rows add up to 1).  This is convenient because one can then do an inner (dot) product of rows to get a cosine similarity measure that varies between -1 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2ecdcc-4f3c-4d68-8413-70567f205c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = tf * idf\n",
    "tfidf = (tfidf.T / np.sqrt((tfidf.T * tfidf.T).sum(axis=0))).T\n",
    "tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72f5636-c099-4786-840f-ff0c909cda9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dot(tfidf.loc[0], tfidf.loc[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47542821",
   "metadata": {},
   "source": [
    "## Back to the topic modeling\n",
    "\n",
    "We can choose how to convert our posts and words into a document-term matrix.  Let's use TF-IDF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f76fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e559aabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "vectors_train = vectorizer.fit_transform(ntrain.data)\n",
    "vectors_test = vectorizer.transform(ntest.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a92d50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6668f3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(vectors_train.toarray(),\n",
    "                  columns=vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc6702f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ce2186",
   "metadata": {},
   "source": [
    "What do the number of rows and columns in the dataframe represent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958a01cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['earth','graphics','image','nasa','algorithms','astronomy']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97fc034",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73caaccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MultinomialNB()\n",
    "clf.fit(vectors_train, ntrain.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426d3ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = clf.predict(vectors_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947fb773",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e23da64",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.accuracy_score(ntest.target, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546c9856",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(ntest.target, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0b57f5",
   "metadata": {},
   "source": [
    "So good!\n",
    "\n",
    "Too good??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16774a45",
   "metadata": {},
   "source": [
    "Let's look at the words that have the highest probability for each of these two classes.\n",
    "\n",
    "We can get the feature names with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d7c6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdee9008",
   "metadata": {},
   "source": [
    "And we can get the probabilities (actually log of probabilities because they're so small) with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a209ff34",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.feature_log_prob_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f37ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.feature_log_prob_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5122dbaf",
   "metadata": {},
   "source": [
    "There are two rows and 23882 columns because this shows *P(class | word)* for each class and each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe11424",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75cc5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_top10(classifier, vectorizer, categories):\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    for i,category in enumerate(categories):\n",
    "        top10 = np.argsort(-classifier.feature_log_prob_[i])[:10]\n",
    "        print('%s:' % (category))\n",
    "        for j in top10:\n",
    "            print(\"%s: %.2f\" % (feature_names[j], classifier.feature_log_prob_[i][j]))\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70935ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_top10(clf, vectorizer, ntrain.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0b27dc",
   "metadata": {},
   "source": [
    "We could stand to do more data processing.\n",
    "\n",
    "First, remove all the meta-data in the headers and footers, along with quotes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5465d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ntrain = fetch_20newsgroups(subset='train',categories=cats,remove=('headers','footers','quotes'))\n",
    "ntest = fetch_20newsgroups(subset='test',categories=cats,remove=('headers','footers','quotes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640404b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = vectorizer.fit_transform(ntrain.data)\n",
    "vectors_test = vectorizer.transform(ntest.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ddac0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MultinomialNB()\n",
    "clf.fit(vectors, ntrain.target)\n",
    "pred = clf.predict(vectors_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23907c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(ntest.target, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47087b3",
   "metadata": {},
   "source": [
    "The performance dropped a little, but likely because we aren't overfitting to words in the parts of the post that aren't as relevant to our meaningful newsgroup content.\n",
    "\n",
    "How do the word probabilities change?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b691af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_top10(clf, vectorizer, ntrain.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb23794",
   "metadata": {},
   "source": [
    "Still have a lot of fluff.\n",
    "\n",
    "We can remove stopwords.  The TfidfVectorizer will also allow us to remove words that are really really common, as well as requiring words to occur at least a certain minimum number of times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f963421",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words='english', max_df=0.5, min_df=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b985ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = vectorizer.fit_transform(ntrain.data)\n",
    "vectors_test = vectorizer.transform(ntest.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6896d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MultinomialNB()\n",
    "clf.fit(vectors, ntrain.target)\n",
    "pred = clf.predict(vectors_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f4367f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(ntest.target, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a601f693",
   "metadata": {},
   "source": [
    "The performance did not drop; it may even have slightly improved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a09dcc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_top10(clf, vectorizer, ntrain.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08daed1c",
   "metadata": {},
   "source": [
    "That is looking like a much more meaningful set of words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53104167",
   "metadata": {},
   "source": [
    "Let me try to make a prediction on a new (hypothetical) newsgroup post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86eac829",
   "metadata": {},
   "outputs": [],
   "source": [
    "mypost = '''\n",
    "I was sitting outside with my cat, looking up at the sky through our telescope, \n",
    "when lo and behold, to my little set of eyes I spied an anomalous signal emanating \n",
    "from a distant galaxy.  I've scoured my astronomy books and can't find any \n",
    "description of what I saw.  Is this a new type of celestial phenomenon? or, \n",
    "dare I say it, something extraterrestrial in origin?\n",
    "\n",
    "Please let me know if you're up for sharing insights on my dataset.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1856a87",
   "metadata": {},
   "source": [
    "Do remember that if you want to make a prediction on a new data point, you'll need to transform it in the same way that you transformed your training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc866d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.transform([mypost])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a508fc33",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.transform([mypost]).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941ff840",
   "metadata": {},
   "source": [
    "So many zeroes, we can't really see much in looking at our vector representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2f9908",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.predict(vectorizer.transform([mypost]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528c467f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ntrain.target_names[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d2d954",
   "metadata": {},
   "source": [
    "After all the transforming, predictions, and decoding, indeed we find that the approach correctly classifies mypost as belonging to sci.space.\n",
    "\n",
    ".... though with a certain probability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2b5219",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.predict_proba(vectorizer.transform([mypost]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
