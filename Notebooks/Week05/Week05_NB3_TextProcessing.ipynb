{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5508d0c",
   "metadata": {},
   "source": [
    "# How can we think of text as numbers for quantitative analysis?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e55b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494c6a64",
   "metadata": {},
   "source": [
    "## Bag-of-Words (BoW)\n",
    "\n",
    "BoW represents a document as a set of words without regard for word order.  Each word is assigned a unique index, and a document is represented as a vector whose values at the index for each word are the word counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66faa9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\"The cat slept and then meowed.\", \n",
    "          \"The tiger slept and then roared.\", \n",
    "          \"The boy ran home and then the boy laughed.\"]\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "X = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0782a1dd",
   "metadata": {},
   "source": [
    "Even though we are using Scikit-Learn to do the CountVectoriz-ing, there is no reason that we couldn't manually do it ourselves too with a bit of Python.  It's just convenient to do it the Scikit-Learn way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6495a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4589cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(X.toarray(), \n",
    "             columns=vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8342f9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# as to compare against our corpus:\n",
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66c767b",
   "metadata": {},
   "source": [
    "## Term Frequency-Inverse Document Frequency (TF-IDF)\n",
    "\n",
    "TF-IDF extends BoW by accounting for the uniqueness of words in distinguishing between documents.  The word counts of BoW are weighted by words' relative rarity across the entire corpus.\n",
    "\n",
    "* Scikit-Learn's TF-IDF calculation is [described here](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html#sklearn.feature_extraction.text.TfidfTransformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca2c311",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "X_tfidf = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d091825",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(X_tfidf.toarray(), \n",
    "             columns=vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609e0fcb",
   "metadata": {},
   "source": [
    "There are a lot of mathematical details that come in here for trying to get well behaved forms of TF-IDF, and it's actually a messy business trying to back this out from the word counts and frequencies.\n",
    "\n",
    "You can ignore the following if you want to, but here is how one would go directly from the matrix of counts to scikit-learn's version of the TFIDF measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbbb0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_bow = pd.DataFrame(X.toarray(), \n",
    "             columns=vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f832d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb39a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the term frequencies in each of the three documents\n",
    "(x_bow.T / x_bow.T.sum(axis=0)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d72b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the number of documents in which each word occurs\n",
    "(x_bow > 0).sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fe0a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = (x_bow.T / x_bow.T.sum(axis=0)).T\n",
    "\n",
    "# the +1 at the end is so that even words that occur across all docs\n",
    "# still have a non-zero TFIDF\n",
    "# the +1 in numerator and +1 in denominator are conveniences to\n",
    "# handle the otherwise division by 0 for words that have 0 counts\n",
    "idf = np.log((1+3) / (1+(x_bow > 0).sum(axis=0))) + 1\n",
    "\n",
    "tf * idf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0621906",
   "metadata": {},
   "source": [
    "... and then one has to do a cosine normalization (the squares of elements in the rows add up to 1).  This is convenient because one can then do an inner (dot) product of rows to get a cosine similarity measure that varies between -1 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef0045c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = tf * idf\n",
    "tfidf = (tfidf.T / np.sqrt((tfidf.T * tfidf.T).sum(axis=0))).T\n",
    "tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5443d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dot(tfidf.loc[0], tfidf.loc[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a416d7d",
   "metadata": {},
   "source": [
    "## Word Embeddings\n",
    "\n",
    "Word embeddings represent words as dense vectors in a continuous vector space. Word2Vec, GloVe, or FastText are pre-trained word embedding models that can be used to help obtain word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e5a6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_corpus = [word_tokenize(sentence.lower()) for sentence in corpus]\n",
    "\n",
    "model = Word2Vec(sentences=tokenized_corpus, \n",
    "                 vector_size=2,\n",
    "                 min_count=1)\n",
    "\n",
    "word_vectors = model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cddde7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e480c244",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors.index_to_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2e27b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors['cat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3540fe58",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_for_document = [word_vectors[word] for word in tokenized_corpus[0] if word in word_vectors.index_to_key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e350a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_for_document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c32940d",
   "metadata": {},
   "source": [
    "The dense vectors can allow us to look for similarity scores, e.g., by looking at the inner (dot) product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962097a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dot(word_vectors['cat'], word_vectors['meowed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc00e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dot(word_vectors['cat'], word_vectors['tiger'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fd14c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dot(word_vectors['cat'], word_vectors['the'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d186d8e4",
   "metadata": {},
   "source": [
    "# Word embedding plotting example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1cb04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors.index_to_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283401ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings = {word: model.wv[word] for word in word_vectors.index_to_key}\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "for word, wordvec in word_embeddings.items():\n",
    "  ax.scatter(wordvec[0], wordvec[1])\n",
    "  ax.annotate(word, (wordvec[0], wordvec[1]))\n",
    "\n",
    "plt.xlabel(\"Component 1\")\n",
    "plt.ylabel(\"Component 2\")\n",
    "plt.title(\"Word Embeddings in 2D Space\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b5c892",
   "metadata": {},
   "source": [
    "In the above, the \"2\" dimensions may be reasonable for plotting, but it's a dramatic projection of a high-dimensional space into a lower dimensional space for visualization.\n",
    "\n",
    "When the texts become really large, the problem becomes even more dramatic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1426cac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the text of \"Moby Dick\"\n",
    "from nltk.corpus import gutenberg\n",
    "moby_dick_text = gutenberg.raw('melville-moby_dick.txt')\n",
    "\n",
    "# Sentence Tokenization\n",
    "sentences = sent_tokenize(moby_dick_text)\n",
    "words = word_tokenize(moby_dick_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6691bd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2598de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f63638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only uncomment this if you want lots of output\n",
    "# moby_dick_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4304c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences[55:56]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9da3566",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_corpus = [word_tokenize(sentence.lower()) for sentence in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea787214",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_corpus[55:56]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd8ec73",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(sentences=tokenized_corpus, \n",
    "                 vector_size=100,\n",
    "                 min_count=1)\n",
    "\n",
    "word_vectors = model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7814fafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.similarity('woman', 'man')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f691de",
   "metadata": {},
   "source": [
    "The similarity score is the cosine between the vectors representing the word embeddings.  The full word-document matrix is 255028-dimensional, while the word-embedding is only 100-dimensional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9de7e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dot(model.wv['woman'], \n",
    "       model.wv['man']) / (np.linalg.norm(model.wv['woman']) * \n",
    "                           np.linalg.norm(model.wv['man']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb537e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.similarity('sea', 'scarcity')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d819959a",
   "metadata": {},
   "source": [
    "# Contextual embeddings\n",
    "\n",
    "Contextualized embeddings consider the surrounding words in a sentence.  As examples:\n",
    "\n",
    "* BERT (Bidirectional Encoder Representations from Transformers) is a pre-trained model that can be used to help obtain embeddings for words, sentences, and documents.\n",
    "* GPT (Generative Pre-trained Transformer) also works with contextual embeddings and context-dependent representations of words.\n",
    "\n",
    "Transformers take us into the realm of deep learning, which we haven't touched on yet, but which we'll return to in just a few weeks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
