{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4edc1037-9b05-44c6-8c07-219f69328b77",
   "metadata": {},
   "source": [
    "# Topic modeling\n",
    "\n",
    "We are going to look at data from the [20 Newsgroups](http://qwone.com/~jason/20Newsgroups/) dataset.  These are postings to newsgroups in 20 different categories.  We will focus on 3 to keep things simple (and the computations quick).\n",
    "\n",
    "Scikit-learn has a function for downloading the data.  See: https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_20newsgroups.html\n",
    "\n",
    "**Remember: We are using labeled data but exploring this from the standpoint of unsupervised learning.  It is convenient to use the data because we know that the topics naturally fall into discrete thematic groups, but that information is not passed to the models, nor would we assume it to be known beforehand.**\n",
    "\n",
    "## LDA\n",
    "\n",
    "Latent Dirichlet Allocation:  a topic model that generates topics based on a set of documents' word frequencies.\n",
    "\n",
    "* Get a \"dictionary\" that has IDs for all the words along with a record of their word frequencies.\n",
    "* Use our \"bag of words\" to generate a list for each document containing its words and their frequencies\n",
    "* Use gensim to generate an LDA model\n",
    "\n",
    "## Gensim\n",
    "\n",
    "* \"Gensim is an open-source library for unsupervised topic modeling and natural language processing, using modern statistical machine learning.\"\n",
    "* [gensim website](https://radimrehurek.com/gensim/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011b5d42-d37e-4e71-ab13-d99e91048d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b361ee-d241-4bf7-b3ef-0a32dcc2e62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will be reminiscent of last week\n",
    "# but we'll also bring along the rec.motorcycles data\n",
    "\n",
    "cats = ['sci.space', 'comp.graphics', 'rec.motorcycles']\n",
    "\n",
    "data = fetch_20newsgroups(categories=cats,\n",
    "                          remove=('headers','footers','quotes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad16c2c1-2207-4233-8281-307161fadbed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(data.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73716a29-da6e-41cb-8436-ed5672d03e06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = data.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae459b3-e221-441d-81c1-02ea865a25ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0267475a-529a-48cc-a91d-b3fde92af8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc714e58-530e-4693-be86-97f4ff144588",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa66e72-9d94-43dd-977a-c73fedab972d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7924c31b-452b-437a-a765-08e932ccec12",
   "metadata": {},
   "source": [
    "We use NLTK to pre-process the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bae2b9-1a9b-4a0d-9b72-2ba9c42c8580",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "\n",
    "# getting corpora\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512c234a-f674-40a7-8311-e21aa308148b",
   "metadata": {},
   "outputs": [],
   "source": [
    "myStopWords = list(punctuation) + stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac2b161-d260-4655-a5eb-d4907f18e09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab39c868-8420-4873-9991-1765a22aab75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "[w for w in word_tokenize(x[0].lower()) if w not in myStopWords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a948b9-75b6-424e-94b3-6c16700b2287",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []\n",
    "for i in x:\n",
    "    docs.append([w for w in word_tokenize(i.lower()) if w not in myStopWords])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d53c1a8-cb62-4119-94bb-178619912b52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b80233-5cc9-43f1-a2ed-d69815f39d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "#from nltk.stem import LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2e3130-fb17-4e22-b775-bf6e98f878fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create p_stemmer of class PorterStemmer\n",
    "p_stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e877102-13db-43ed-82d1-551ec474d977",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_stemmed = []\n",
    "for i in docs:\n",
    "    docs_stemmed.append([p_stemmer.stem(w) for w in i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5436a74-4f45-4e7d-96ca-12e7ef806c45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "docs_stemmed[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6505fe7-86b4-4bb2-9827-031236c16d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ....hmm.....\n",
    "# there's still a lot of junk, so rather than stemming, let's\n",
    "# try to retain only the \"valid\" words\n",
    "\n",
    "import nltk\n",
    "nltk.download('words')\n",
    "nltk_valid_words = set(nltk.corpus.words.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c3477a-9b6d-4c98-9225-592cefbc8251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [this will still not be a perfect method]\n",
    "'nasa' in nltk_valid_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e211dc-f2c0-4df8-b3e4-f98b2663fdc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "'algorithm' in nltk_valid_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac0bff3-96a2-4f9a-b98c-0072e843d2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "'algorithms' in nltk_valid_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ae2215-b64d-4d74-b4f9-d9e2d963de88",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_validwords = []\n",
    "for d in docs: #_stemmed:\n",
    "    docs_validwords.append([i for i in d if i in nltk_valid_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba92a913-94db-4d4c-bd86-70bfc8fe922c",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbeb650-ce00-45bd-a14e-6a545150a620",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_validwords[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8092a60b-32b1-4173-aa67-794881638608",
   "metadata": {},
   "source": [
    "Here we use gensim to make the dictionary and corpus structures, and to employ the LDA model to extract groups (aka topics) and the distribution of words for each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2294d9-ab78-42e0-83ca-321bc381503d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da8cbe2-dc2f-495a-b8b5-983e296d9b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary = corpora.Dictionary(docs_stemmed)\n",
    "dictionary = corpora.Dictionary(docs_validwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde8095b-24a9-49c7-8f10-a224696616e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c28ea8a-5962-49e1-aa9d-2f14900865ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(no_below=10, no_above=0.5)\n",
    "# could also trim with keep_n=1000 or similar to keep only the top words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0932895-df99-4a9b-be70-039fdc2a50ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d17faa4-eca1-4647-a0f7-66163435e2b4",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679a4c55-b149-47c6-ac03-8d007b2e7e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dictionary.token2id['algorithm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b986523-2eb2-4555-935a-5de37eb9f84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary[36]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa8266d-bb5d-4626-95a1-91d2df9dd084",
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus = [dictionary.doc2bow(text) for text in docs_stemmed]\n",
    "corpus = [dictionary.doc2bow(text) for text in docs_validwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5383fd9b-0267-4d5c-ab54-eb06e8c082bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac82bafd-dccc-4697-9680-503f0d736573",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dictionary.token2id['science'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1baa1f-ba7d-4242-bec7-0d95fb484e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary[27]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de5415e-1ef5-477c-b7a2-5ce47d6e4e36",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# docs_stemmed[0]\n",
    "docs_validwords[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284854d0-92a4-482e-937e-904e93e21be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordid = corpus[0][27]\n",
    "print(dictionary[wordid[0]],wordid[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5282e3-5ec2-47d0-b5b6-2875af2e5360",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in corpus[0]:\n",
    "    print(dictionary[i[0]], i[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f364d19a-d545-485b-bb5b-25be3cfba874",
   "metadata": {},
   "outputs": [],
   "source": [
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, \n",
    "                                           num_topics=3, \n",
    "                                           id2word = dictionary, \n",
    "                                           passes=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01662301-327f-421a-9908-83ab0c969cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "ldamodel.show_topics(num_topics=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34abdd71-d58e-447b-a5c1-94e91090ed73",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in ldamodel.print_topics(num_topics=3, num_words=20):\n",
    "    print(i[0])\n",
    "    print(i[1])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d701e5-1f5f-484e-bdd1-7bb2e5ab634a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f68fcae-e88b-4759-bf47-393a9577f037",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e75458a-89a5-4535-83b0-bcfcdf8472b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "re.split(re.escape(' + ') + '|' + re.escape('*'), 'hi + me*4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190467bd-c55d-43e2-96f1-4026e4a346e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,3,figsize=(8,6))\n",
    "for i in ldamodel.print_topics(num_topics=3, num_words=20):\n",
    "    x = []\n",
    "    y = []\n",
    "    count = 0\n",
    "    for j in re.split(re.escape(' + ') + '|' + re.escape('*'), i[1]):\n",
    "        if count % 2 == 0:\n",
    "            y.insert(0,float(j))\n",
    "        else:\n",
    "            x.insert(0,j)\n",
    "        count += 1\n",
    "    ax[i[0]].barh(x,y,height=0.5)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b67a169-d4ed-438d-ac19-d3f7302beb73",
   "metadata": {},
   "source": [
    "# TF-IDF (Term Frequency Inverse Document Frequency)\n",
    "\n",
    "TF-IDF is similar to bag-of-words, but it down weights words appearing frequently across lots of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3416752c-d9ac-455b-b982-ddc7cce0923a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize the model\n",
    "tfidf = gensim.models.TfidfModel(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d709d9-a14c-408e-b2e1-67d82583e45b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc23d80-e01c-40cd-964c-2aa077283263",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# apply transformation\n",
    "tfidf[corpus[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6056c0-7920-47fc-a533-9bc99e2eb4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_transformed = tfidf[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4e8ef0-9314-4eae-9827-e155bbdcb4c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "corpus_transformed[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5c1c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf.num_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287fdb77-fab9-46b2-8e21-d0a315adff8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ldamodel_tfidf = gensim.models.ldamodel.LdaModel(corpus_transformed, \n",
    "                                           num_topics=3, \n",
    "                                           id2word = dictionary, \n",
    "                                           passes=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e572e6e-13cb-4e07-a360-ede842fd5d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in ldamodel_tfidf.print_topics(num_topics=3, num_words=20):\n",
    "    print(i[0])\n",
    "    print(i[1])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649a45f0-4dec-4241-a167-24ac08a912fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,3,figsize=(8,6))\n",
    "for i in ldamodel_tfidf.print_topics(num_topics=3, num_words=20):\n",
    "    x = []\n",
    "    y = []\n",
    "    count = 0\n",
    "    for j in re.split(re.escape(' + ') + '|' + re.escape('*'), i[1]):\n",
    "        if count % 2 == 0:\n",
    "            y.insert(0,float(j))\n",
    "        else:\n",
    "            x.insert(0,j)\n",
    "        count += 1\n",
    "    ax[i[0]].barh(x,y,height=0.5)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18de454b-2539-404d-80fa-16d14b6f36e5",
   "metadata": {},
   "source": [
    "# Scikit-Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f955694-5f18-4a08-8474-629ba70d45f7",
   "metadata": {},
   "source": [
    "Returing to our handy Scikit-Learn library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6305b2-f6c5-4415-ba27-a8ba9fe03d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "cats = ['sci.space', 'comp.graphics', 'rec.motorcycles']\n",
    "data = fetch_20newsgroups(categories=cats,\n",
    "                   remove=('headers','footers','quotes'))\n",
    "corpus = data.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c0081b-901e-48d7-9095-b3293ed6a9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f27fba-2a06-4765-bd03-78ec0e43a25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a61036-4e54-4efe-a1ad-d5e0f6b63f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2957b98-251c-4685-99df-0b5bc36371a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the document-term matrix\n",
    "\n",
    "# use CountVectorizer to convert the text to a matrix of token counts\n",
    "# min_frac: words occurring in less than the minimum fraction of documents \n",
    "#           are excluded; integer refers to count rather than fraction\n",
    "# max_frac: words occurring in more than the maximum fraction of documents can be included\n",
    "#           are excluded\n",
    "# max_words: after exclusions, the most commonly occurring words are retained\n",
    "#           up to a total of max_words words\n",
    "# ngram_range: can be used to include pairs, triples, etc of words for larger\n",
    "#           ranges like (1,2), (1,3), etc\n",
    "# stop_words: pass in the list of stop words to be dropped\n",
    "#           which here is ignored\n",
    "\n",
    "min_frac    = 2\n",
    "max_frac    = 0.75\n",
    "max_words   = 5000\n",
    "ngram_range = (1,2)\n",
    "stop_words  = 'english'\n",
    "\n",
    "vectorizer = CountVectorizer(\n",
    "    min_df=min_frac, \n",
    "    max_df=max_frac, \n",
    "    max_features=max_words, \n",
    "    ngram_range=ngram_range,\n",
    "    stop_words=stop_words\n",
    ")\n",
    "# vectorizer = TfidfVectorizer(\n",
    "#     min_df=min_frac, \n",
    "#     max_df=max_frac, \n",
    "#     max_features=max_words, \n",
    "#     ngram_range=ngram_range,\n",
    "#     stop_words=stop_words\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153e3452-aa8f-49b8-9604-2d45f0c60771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and transform the corpus texts into a word frequency matrix\n",
    "word_freq_matrix = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Convert the word frequency matrix to a numerical (numpy) array\n",
    "word_freq_array = word_freq_matrix.toarray()\n",
    "\n",
    "# Get the words for column names\n",
    "words = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Create a dataframe of the result (not currently used below)\n",
    "word_freq_df = pd.DataFrame(word_freq_array, columns=words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1292081-a19b-41e1-9761-786d715ac585",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83150afe-b820-4ccc-a47d-38bfc2e29458",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq_df[['earth','graphics','image',\n",
    "              'nasa','algorithms','astronomy',\n",
    "              'bike','dirt','motorcycle']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6536cb-5631-46eb-9295-dd424a76b68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the LDA model\n",
    "# \n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html\n",
    "# \n",
    "# initialize the LDA object\n",
    "# n_components: the number of topics\n",
    "# max_iter: Number of iterations over all the training data during training\n",
    "# n_jobs: Number of parallel task to use during training (-1 for all available cores)\n",
    "# learning_method: can be \"batch\" or \"online\"\n",
    "#     batch uses all training data in each EM update\n",
    "#     online uses mini-batch of training data in each EM update\n",
    "#            The learning rate is controlled by the learning_decay \n",
    "#            and the learning_offset parameters\n",
    "# learning_decay: should be set between (0.5, 1.0] to guarantee asymptotic convergence\n",
    "# learning_offset: A (positive) parameter that downweights early iterations in online learning\n",
    "#                  should be > 1.0\n",
    "# random_state: sets the random number seed for reproducibility\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=3,\n",
    "                                max_iter=200,\n",
    "                                n_jobs=-1,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                learning_decay=.5,\n",
    "                                random_state=0,\n",
    "                                evaluate_every=5, # [by default, perplexity change of < 0.1 is used as check for convergence]\n",
    "                                verbose=1)\n",
    "\n",
    "# Training the LDA model\n",
    "lda.fit(word_freq_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ca5473-78b3-45a4-a705-8acaf63d244b",
   "metadata": {},
   "outputs": [],
   "source": [
    "numtopics = 3\n",
    "\n",
    "# document-by-topic matrix\n",
    "doc_topic_matrix = lda.transform(word_freq_matrix)\n",
    "\n",
    "# Convert the document-topic matrix to a dataframe (tabular structure)\n",
    "doc_topic_df = pd.DataFrame(doc_topic_matrix,\n",
    "                            columns=(['topic_' + str(i) for i in range(numtopics)]))\n",
    "\n",
    "# Look at it\n",
    "# Entries are the probability of the document belonging to the topic\n",
    "doc_topic_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd87f67e-b8e4-4b7d-9fa8-91d552c2e47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic-by-word matrix\n",
    "topic_word_matrix = lda.components_\n",
    "\n",
    "# Convert the topic-word matrix to a dataframe (tabular structure)\n",
    "topic_word_df = pd.DataFrame(topic_word_matrix, columns=words)\n",
    "\n",
    "# Look at it\n",
    "# Entries are NOT probabilities, but they tell you importances of the word-topic correspondence\n",
    "topic_word_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a173cc97-cbd6-4bff-881a-3acd7000c4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_word_df[['earth','graphics','image',\n",
    "               'nasa','algorithms','astronomy',\n",
    "               'bike','dirt','motorcycle']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45e855f-760f-4f3c-9086-62b9e3ed0ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# interpretation\n",
    "\n",
    "# document-by-topic matrix\n",
    "doc_topic_matrix = lda.transform(word_freq_array)\n",
    "\n",
    "# topic-by-word matrix\n",
    "topic_word_matrix = lda.components_\n",
    "\n",
    "# top words in each topic\n",
    "n_top_words = 10\n",
    "for topic_idx, topic in enumerate(topic_word_matrix):\n",
    "    message = \"Topic #%d: \" % topic_idx\n",
    "    message += \" \".join([words[i]\n",
    "                         for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "    print(message)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0591546-b58d-4c47-825b-1d2f6891055d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will scan across number of topics from 10 to 50 in steps of 5\n",
    "# and make a plot showing the decrease in perplexity score \n",
    "# as a function of number of topics\n",
    "\n",
    "# train LDA models with different numbers of topics\n",
    "numtopics = range(1,10)\n",
    "perplexityscores = []\n",
    "for i in numtopics:\n",
    "    print('Finished training with topics =', i)\n",
    "    lda = LatentDirichletAllocation(n_components=i,\n",
    "                                    max_iter=200,\n",
    "                                    n_jobs=-1,\n",
    "                                    learning_method='online',\n",
    "                                    learning_offset=50.,\n",
    "                                    learning_decay=.5,\n",
    "                                    random_state=0)\n",
    "    \n",
    "    lda.fit(word_freq_array)\n",
    "    \n",
    "    perplexityscores.append(lda.perplexity(word_freq_array))\n",
    "    \n",
    "plt.plot(numtopics, perplexityscores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd0e7c6-0447-4140-8d96-5ffd192d8f4d",
   "metadata": {},
   "source": [
    "# Visualization for NLP\n",
    "\n",
    "Here we'll use a new-ish library for topic model visualizatin: topicwizard\n",
    "* https://x-tabdeveloping.github.io/topicwizard/\n",
    "* Some things may not play nicely if you try this with other datasets/models, but it's definitely worth exploring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef571e8f-01f5-4152-83e6-dc936812c1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can install with pip, e.g.\n",
    "# !pip install topic-wizard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8475f84c-66cc-4873-87e8-5835ccdacac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import topicwizard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26eef46-dee5-4bf1-9757-92848131b05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(\n",
    "    min_df=min_frac, \n",
    "    max_df=max_frac, \n",
    "    max_features=max_words, \n",
    "    ngram_range=ngram_range,\n",
    "    stop_words=stop_words\n",
    ")\n",
    "\n",
    "model = LatentDirichletAllocation(n_components=3,\n",
    "                                max_iter=200,\n",
    "                                n_jobs=-1,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                learning_decay=.5,\n",
    "                                random_state=0)\n",
    "\n",
    "topic_pipeline = topicwizard.pipeline.make_topic_pipeline(vectorizer, model)\n",
    "\n",
    "topic_pipeline.fit(corpus)\n",
    "\n",
    "topicwizard.visualize(corpus, model=topic_pipeline)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
